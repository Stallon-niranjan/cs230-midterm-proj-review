{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dghosh/anaconda3/envs/deeplearn/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 112, 112, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 112, 112, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 56, 56, 64)   4160        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 56, 56, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 56, 56, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 56, 56, 256)  16640       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 56, 56, 256)  1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 56, 56, 256)  0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 56, 56, 256)  0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 56, 56, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 56, 56, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 56, 56, 256)  0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 56, 56, 256)  0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 56, 56, 64)   16448       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 56, 56, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 56, 56, 64)   36928       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 56, 56, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 56, 56, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 56, 56, 256)  16640       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 56, 56, 256)  1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 56, 56, 256)  0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 56, 56, 256)  0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 28, 28, 128)  32896       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 28, 28, 128)  0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 28, 28, 128)  0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 28, 28, 512)  131584      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 28, 28, 512)  2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 28, 28, 512)  0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 28, 28, 512)  0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 28, 28, 128)  0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 28, 28, 128)  0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 28, 28, 512)  0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 28, 28, 512)  0           add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 28, 28, 128)  0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 28, 28, 128)  0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 28, 28, 512)  0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 28, 28, 512)  0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 28, 28, 128)  65664       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 28, 28, 128)  0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 28, 28, 128)  147584      activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 28, 28, 128)  512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 28, 28, 128)  0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 28, 28, 512)  66048       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 28, 28, 512)  2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 28, 28, 512)  0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 28, 28, 512)  0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 14, 14, 256)  131328      activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 14, 14, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 14, 14, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 14, 14, 1024) 525312      activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 14, 14, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 14, 14, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 14, 14, 1024) 0           add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 14, 14, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 14, 14, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 14, 14, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 14, 14, 1024) 0           add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 14, 14, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 14, 14, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 14, 14, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 14, 14, 1024) 0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 14, 14, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 14, 14, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 14, 14, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 14, 14, 1024) 0           add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 14, 14, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 14, 14, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 14, 14, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 14, 14, 1024) 0           add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 14, 14, 256)  262400      activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 14, 14, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 14, 14, 256)  590080      activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 14, 14, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 14, 14, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 14, 14, 1024) 263168      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 14, 14, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 14, 14, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 14, 14, 1024) 0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 7, 7, 512)    524800      activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 7, 7, 512)    0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 7, 7, 2048)   2099200     activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 7, 7, 2048)   8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 7, 7, 2048)   0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 7, 7, 2048)   0           add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 7, 7, 512)    0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 7, 7, 2048)   0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 7, 7, 2048)   0           add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 7, 7, 512)    1049088     activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 7, 7, 512)    2359808     activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 7, 7, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 7, 7, 512)    0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)         (None, 7, 7, 2048)   1050624     activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizati (None, 7, 7, 2048)   8192        res5c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 7, 7, 2048)   0           bn5c_branch2c[0][0]              \n",
      "                                                                 activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 7, 7, 2048)   0           add_71[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# approach 2 - THIS IS THE CORRECT ONE\n",
    "from keras.applications import ResNet50\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "BATCH_SIZE = 100\n",
    "NUM_CLASSES = 46\n",
    "TRAIN_DATA_SIZE = 20000\n",
    "TEST_DATA_SIZE = 10000\n",
    "VAL_DATA_SIZE = 10000\n",
    "img_h = 224\n",
    "img_w = 224\n",
    "\n",
    "conv_base = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(img_h, img_w, 3))\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209215 images belonging to 46 classes.\n",
      "batch=1, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=2, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=3, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=4, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=5, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=6, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=7, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=8, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=9, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=10, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=11, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=12, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=13, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=14, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=15, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=16, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=17, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=18, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=19, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=20, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=21, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=22, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=23, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=24, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=25, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=26, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=27, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=28, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=29, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=30, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=31, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=32, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=33, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=34, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=35, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=36, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=37, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=38, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=39, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=40, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=41, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=42, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=43, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=44, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=45, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=46, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=47, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=48, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=49, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=50, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=51, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=52, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=53, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=54, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=55, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=56, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=57, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=58, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=59, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=60, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=61, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=62, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=63, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=64, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=65, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=66, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=67, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=68, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=69, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=70, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=71, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=72, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=73, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=74, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=75, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=76, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=77, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=78, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=79, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=80, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=81, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=82, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=83, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=84, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=85, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=86, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=87, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=88, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=89, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=90, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=91, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=92, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=93, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=94, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=95, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=96, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=97, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=98, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=99, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=100, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=101, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=102, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=103, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=104, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=105, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=106, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=107, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=108, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=109, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=110, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=111, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=112, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=113, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=114, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=115, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=116, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=117, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=118, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=119, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=120, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=121, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=122, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=123, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=124, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=125, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=126, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=127, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=128, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=129, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=130, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=131, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=132, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=133, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=134, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=135, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=136, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=137, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=138, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=139, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=140, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=141, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=142, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=143, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=144, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=145, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=146, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=147, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=148, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=149, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=150, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=151, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=152, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=153, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=154, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=155, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=156, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=157, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=158, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=159, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=160, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=161, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=162, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=163, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=164, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=165, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=166, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=167, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=168, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=169, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=170, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=171, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=172, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=173, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=174, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=175, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=176, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=177, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=178, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=179, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=180, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=181, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=182, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=183, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=184, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=185, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=186, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=187, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=188, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=189, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=190, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=191, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=192, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=193, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=194, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=195, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=196, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=197, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=198, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=199, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "batch=200, features dim=(20000, 7, 7, 2048), labels_dim=(20000, 46)\n",
      "Found 39999 images belonging to 46 classes.\n",
      "batch=1, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=2, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=3, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=4, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=5, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=6, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=7, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=8, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=9, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=10, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=11, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=12, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=13, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=14, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=15, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=16, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=17, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=18, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=19, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=20, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=21, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=22, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=23, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=24, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=25, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=26, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=27, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=28, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=29, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=30, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=31, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=32, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=33, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=34, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=35, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=36, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=37, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=38, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=39, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=40, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=41, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=42, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=43, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=44, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=45, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=46, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=47, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=48, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=49, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=50, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=51, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=52, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=53, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=54, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=55, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=56, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=57, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=58, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=59, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=60, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=61, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=62, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=63, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=64, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=65, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=66, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=67, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=68, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=69, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=70, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=71, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=72, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=73, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=74, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=75, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=76, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=77, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=78, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=79, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=80, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=81, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=82, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=83, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=84, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=85, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=86, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=87, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=88, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=89, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=90, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=91, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=92, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=93, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=94, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=95, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=96, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=97, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=98, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=99, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=100, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "Found 39998 images belonging to 46 classes.\n",
      "batch=1, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=2, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=3, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=4, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=5, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=6, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=7, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=8, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=9, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=10, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=11, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=12, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=13, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=14, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=15, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=16, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=17, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=18, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=19, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=20, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=21, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=22, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=23, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=24, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=25, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=26, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=27, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=28, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=29, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=30, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=31, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=32, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=33, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=34, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=35, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=36, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=37, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=38, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=39, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=40, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=41, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=42, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=43, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=44, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=45, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=46, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=47, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=48, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=49, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=50, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=51, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=52, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=53, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=54, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=55, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=56, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=57, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=58, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=59, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=60, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=61, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=62, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=63, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=64, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=65, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch=66, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=67, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=68, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=69, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=70, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=71, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=72, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=73, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=74, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=75, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=76, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=77, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=78, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=79, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=80, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=81, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=82, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=83, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=84, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=85, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=86, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=87, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=88, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=89, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=90, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=91, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=92, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=93, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=94, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=95, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=96, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=97, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=98, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=99, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n",
      "batch=100, features dim=(10000, 7, 7, 2048), labels_dim=(10000, 46)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./deepfashion/\"\n",
    "train_dataset_path=os.path.join(dataset_path, \"dataset\", \"train\")\n",
    "test_dataset_path=os.path.join(dataset_path, \"dataset\", \"test\")\n",
    "val_dataset_path=os.path.join(dataset_path, \"dataset\", \"val\")\n",
    "\n",
    "def extract_features(directory, sample_count, n_H=img_h, n_W=img_w, batch_size=BATCH_SIZE, num_category=NUM_CLASSES):\n",
    "    # since final layer of resnet50 pretrained has 7, 7, 2048 dims\n",
    "    features = np.zeros(shape=(sample_count, 7, 7, 2048))\n",
    "    labels = np.zeros(shape=(sample_count, num_category))\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=(n_H, n_W),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        print(\"batch=%s, features dim=%s, labels_dim=%s\" % (i, features.shape, labels.shape))        \n",
    "        if i * batch_size >= sample_count:\n",
    "            # Note that since generators yield data indefinitely in a loop,\n",
    "            # we must `break` after every image has been seen once.\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dataset_path, 20000)\n",
    "validation_features, validation_labels = extract_features(val_dataset_path, 10000)\n",
    "test_features, test_labels = extract_features(test_dataset_path, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extracted features are currently of shape (samples, 7, 7, 2048). \n",
    "# We will feed them to a densely-connected classifier, so first we must flatten them to (samples, 7*7*2048)\n",
    "\n",
    "TRAIN_DATA_SIZE = 20000\n",
    "TEST_DATA_SIZE = 10000\n",
    "VAL_DATA_SIZE = 10000\n",
    "\n",
    "\n",
    "train_features = np.reshape(train_features, (TRAIN_DATA_SIZE, 7 * 7 * 2048))\n",
    "validation_features = np.reshape(validation_features, (VAL_DATA_SIZE, 7 * 7 * 2048))\n",
    "test_features = np.reshape(test_features, (TEST_DATA_SIZE, 7 * 7 * 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resized features for quick future runs\n",
    "np.save('resnet50_bottleneck_deepfashion_train_20k_100.npy', train_features)\n",
    "np.save('resnet50_bottleneck_deepfashion_val_20k_100.npy', validation_features)\n",
    "np.save('resnet50_bottleneck_deepfashion_test_20k_100.npy', test_features)\n",
    "np.save('resnet50_deepfashion_train_20k_100_labels.npy', train_labels)\n",
    "np.save('resnet50_deepfashion_val_20k_100_labels.npy', validation_labels)\n",
    "np.save('resnet50_deepfashion_test_20k_100_labels.npy', test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               51380736  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 46)                23598     \n",
      "=================================================================\n",
      "Total params: 51,404,334\n",
      "Trainable params: 51,404,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_dim=7 * 7 * 2048))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"./deepfashion/tboard-resnet50-logs/{}\".format(time()), \n",
    "                          write_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.9011 - acc: 0.2335 - val_loss: 2.6450 - val_acc: 0.2795\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.7392 - acc: 0.2634 - val_loss: 2.5920 - val_acc: 0.2906\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 189s 9ms/step - loss: 2.6682 - acc: 0.2759 - val_loss: 2.5250 - val_acc: 0.3047\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.6201 - acc: 0.2857 - val_loss: 2.5311 - val_acc: 0.3085\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.5740 - acc: 0.2949 - val_loss: 2.4954 - val_acc: 0.3022\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 2.5397 - acc: 0.3024 - val_loss: 2.4786 - val_acc: 0.3004\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 2.5148 - acc: 0.3058 - val_loss: 2.4461 - val_acc: 0.3152\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 197s 10ms/step - loss: 2.4814 - acc: 0.3119 - val_loss: 2.4160 - val_acc: 0.3234\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 2.4582 - acc: 0.3164 - val_loss: 2.4008 - val_acc: 0.3246\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 2.4462 - acc: 0.3207 - val_loss: 2.3973 - val_acc: 0.3249\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 2.4246 - acc: 0.3260 - val_loss: 2.3961 - val_acc: 0.3232\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.4056 - acc: 0.3297 - val_loss: 2.3792 - val_acc: 0.3306\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 210s 10ms/step - loss: 2.3929 - acc: 0.3319 - val_loss: 2.3809 - val_acc: 0.3403\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.3730 - acc: 0.3346 - val_loss: 2.3783 - val_acc: 0.3304\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 187s 9ms/step - loss: 2.3657 - acc: 0.3382 - val_loss: 2.3597 - val_acc: 0.3360\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 189s 9ms/step - loss: 2.3521 - acc: 0.3414 - val_loss: 2.3473 - val_acc: 0.3412\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.3332 - acc: 0.3469 - val_loss: 2.3373 - val_acc: 0.3383\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.3251 - acc: 0.3452 - val_loss: 2.3386 - val_acc: 0.3395\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.3094 - acc: 0.3495 - val_loss: 2.3349 - val_acc: 0.3481\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.3035 - acc: 0.3478 - val_loss: 2.3410 - val_acc: 0.3388\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 190s 10ms/step - loss: 2.2979 - acc: 0.3513 - val_loss: 2.3229 - val_acc: 0.3460\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 190s 9ms/step - loss: 2.2805 - acc: 0.3535 - val_loss: 2.3319 - val_acc: 0.3483\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.2755 - acc: 0.3571 - val_loss: 2.3114 - val_acc: 0.3493\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.2718 - acc: 0.3567 - val_loss: 2.3148 - val_acc: 0.3522\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.2590 - acc: 0.3612 - val_loss: 2.3058 - val_acc: 0.3483\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.2480 - acc: 0.3636 - val_loss: 2.2987 - val_acc: 0.3488\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.2371 - acc: 0.3696 - val_loss: 2.3199 - val_acc: 0.3457\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 197s 10ms/step - loss: 2.2301 - acc: 0.3677 - val_loss: 2.2854 - val_acc: 0.3510\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.2249 - acc: 0.3704 - val_loss: 2.2869 - val_acc: 0.3529\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.2155 - acc: 0.3736 - val_loss: 2.2773 - val_acc: 0.3545\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.2104 - acc: 0.3716 - val_loss: 2.2984 - val_acc: 0.3494\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.2015 - acc: 0.3763 - val_loss: 2.2848 - val_acc: 0.3522\n",
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.1941 - acc: 0.3763 - val_loss: 2.2984 - val_acc: 0.3483\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 2.1920 - acc: 0.3765 - val_loss: 2.3107 - val_acc: 0.3563\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.1789 - acc: 0.3799 - val_loss: 2.3123 - val_acc: 0.3321\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.1734 - acc: 0.3814 - val_loss: 2.2657 - val_acc: 0.3635\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.1676 - acc: 0.3855 - val_loss: 2.3026 - val_acc: 0.3458\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.1688 - acc: 0.3860 - val_loss: 2.2762 - val_acc: 0.3517\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 190s 9ms/step - loss: 2.1586 - acc: 0.3857 - val_loss: 2.2871 - val_acc: 0.3535\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 190s 9ms/step - loss: 2.1508 - acc: 0.3859 - val_loss: 2.2671 - val_acc: 0.3538\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.1432 - acc: 0.3866 - val_loss: 2.2642 - val_acc: 0.3633\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.1378 - acc: 0.3916 - val_loss: 2.2568 - val_acc: 0.3654\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.1272 - acc: 0.3920 - val_loss: 2.2880 - val_acc: 0.3520\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 2.1356 - acc: 0.3909 - val_loss: 2.2731 - val_acc: 0.3589\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.1258 - acc: 0.3939 - val_loss: 2.2734 - val_acc: 0.3587\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.1151 - acc: 0.3991 - val_loss: 2.2423 - val_acc: 0.3626\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.1110 - acc: 0.3963 - val_loss: 2.2598 - val_acc: 0.3561\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.1025 - acc: 0.3977 - val_loss: 2.2855 - val_acc: 0.3547\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.1014 - acc: 0.3990 - val_loss: 2.2621 - val_acc: 0.3541\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.0990 - acc: 0.3984 - val_loss: 2.2580 - val_acc: 0.3599\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.0863 - acc: 0.4037 - val_loss: 2.2450 - val_acc: 0.3638\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 190s 10ms/step - loss: 2.0848 - acc: 0.4030 - val_loss: 2.2463 - val_acc: 0.3608\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.0864 - acc: 0.4051 - val_loss: 2.2731 - val_acc: 0.3600\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.0730 - acc: 0.4084 - val_loss: 2.2432 - val_acc: 0.3676\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.0675 - acc: 0.4083 - val_loss: 2.2515 - val_acc: 0.3610\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 2.0605 - acc: 0.4070 - val_loss: 2.2396 - val_acc: 0.3659\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.0571 - acc: 0.4116 - val_loss: 2.2521 - val_acc: 0.3623\n",
      "Epoch 58/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 2.0551 - acc: 0.4113 - val_loss: 2.2560 - val_acc: 0.3599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 2.0435 - acc: 0.4100 - val_loss: 2.2391 - val_acc: 0.3718\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.0415 - acc: 0.4136 - val_loss: 2.2416 - val_acc: 0.3665\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 2.0434 - acc: 0.4136 - val_loss: 2.2398 - val_acc: 0.3725\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 2.0366 - acc: 0.4136 - val_loss: 2.2340 - val_acc: 0.3721\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 2.0349 - acc: 0.4160 - val_loss: 2.2411 - val_acc: 0.3713\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 2.0289 - acc: 0.4186 - val_loss: 2.2641 - val_acc: 0.3629\n",
      "Epoch 65/100\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 2.0171 - acc: 0.4218 - val_loss: 2.2337 - val_acc: 0.3668\n",
      "Epoch 66/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.0129 - acc: 0.4179 - val_loss: 2.2270 - val_acc: 0.3639\n",
      "Epoch 67/100\n",
      "20000/20000 [==============================] - 191s 10ms/step - loss: 2.0121 - acc: 0.4229 - val_loss: 2.2522 - val_acc: 0.3703\n",
      "Epoch 68/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 2.0043 - acc: 0.4229 - val_loss: 2.2421 - val_acc: 0.3719\n",
      "Epoch 69/100\n",
      "20000/20000 [==============================] - 189s 9ms/step - loss: 2.0025 - acc: 0.4236 - val_loss: 2.2400 - val_acc: 0.3706\n",
      "Epoch 70/100\n",
      "20000/20000 [==============================] - 190s 10ms/step - loss: 2.0006 - acc: 0.4260 - val_loss: 2.3066 - val_acc: 0.3412\n",
      "Epoch 71/100\n",
      "20000/20000 [==============================] - 189s 9ms/step - loss: 1.9965 - acc: 0.4253 - val_loss: 2.2227 - val_acc: 0.3742\n",
      "Epoch 72/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 1.9887 - acc: 0.4236 - val_loss: 2.2434 - val_acc: 0.3671\n",
      "Epoch 73/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 1.9833 - acc: 0.4281 - val_loss: 2.2478 - val_acc: 0.3653\n",
      "Epoch 74/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9779 - acc: 0.4278 - val_loss: 2.2618 - val_acc: 0.3579\n",
      "Epoch 75/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9784 - acc: 0.4273 - val_loss: 2.2551 - val_acc: 0.3647\n",
      "Epoch 76/100\n",
      "20000/20000 [==============================] - 190s 10ms/step - loss: 1.9735 - acc: 0.4326 - val_loss: 2.2234 - val_acc: 0.3735\n",
      "Epoch 77/100\n",
      "20000/20000 [==============================] - 190s 9ms/step - loss: 1.9700 - acc: 0.4302 - val_loss: 2.2344 - val_acc: 0.3705\n",
      "Epoch 78/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9625 - acc: 0.4298 - val_loss: 2.2229 - val_acc: 0.3762\n",
      "Epoch 79/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9628 - acc: 0.4331 - val_loss: 2.2365 - val_acc: 0.3720\n",
      "Epoch 80/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 1.9561 - acc: 0.4333 - val_loss: 2.2295 - val_acc: 0.3760\n",
      "Epoch 81/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 1.9585 - acc: 0.4328 - val_loss: 2.2114 - val_acc: 0.3773\n",
      "Epoch 82/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 1.9472 - acc: 0.4364 - val_loss: 2.2353 - val_acc: 0.3750\n",
      "Epoch 83/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 1.9417 - acc: 0.4417 - val_loss: 2.2193 - val_acc: 0.3775\n",
      "Epoch 84/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9396 - acc: 0.4392 - val_loss: 2.2350 - val_acc: 0.3739\n",
      "Epoch 85/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9350 - acc: 0.4412 - val_loss: 2.2276 - val_acc: 0.3755\n",
      "Epoch 86/100\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 1.9354 - acc: 0.4414 - val_loss: 2.2389 - val_acc: 0.3743\n",
      "Epoch 87/100\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 1.9326 - acc: 0.4428 - val_loss: 2.2415 - val_acc: 0.3716\n",
      "Epoch 88/100\n",
      "20000/20000 [==============================] - 197s 10ms/step - loss: 1.9258 - acc: 0.4449 - val_loss: 2.2768 - val_acc: 0.3551\n",
      "Epoch 89/100\n",
      "20000/20000 [==============================] - 190s 10ms/step - loss: 1.9222 - acc: 0.4439 - val_loss: 2.2391 - val_acc: 0.3809\n",
      "Epoch 90/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 1.9162 - acc: 0.4463 - val_loss: 2.2485 - val_acc: 0.3717\n",
      "Epoch 91/100\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 1.9125 - acc: 0.4494 - val_loss: 2.2373 - val_acc: 0.3771\n",
      "Epoch 92/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.9110 - acc: 0.4482 - val_loss: 2.2444 - val_acc: 0.3706\n",
      "Epoch 93/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 1.9079 - acc: 0.4483 - val_loss: 2.2298 - val_acc: 0.3755\n",
      "Epoch 94/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 1.9035 - acc: 0.4492 - val_loss: 2.2113 - val_acc: 0.3818\n",
      "Epoch 95/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.8965 - acc: 0.4526 - val_loss: 2.2232 - val_acc: 0.3742\n",
      "Epoch 96/100\n",
      "20000/20000 [==============================] - 188s 9ms/step - loss: 1.8977 - acc: 0.4490 - val_loss: 2.2338 - val_acc: 0.3749\n",
      "Epoch 97/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.8910 - acc: 0.4510 - val_loss: 2.2282 - val_acc: 0.3757\n",
      "Epoch 98/100\n",
      "20000/20000 [==============================] - 192s 10ms/step - loss: 1.8838 - acc: 0.4563 - val_loss: 2.2083 - val_acc: 0.3847\n",
      "Epoch 99/100\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 1.8850 - acc: 0.4577 - val_loss: 2.2247 - val_acc: 0.3822\n",
      "Epoch 100/100\n",
      "20000/20000 [==============================] - 194s 10ms/step - loss: 1.8825 - acc: 0.4560 - val_loss: 2.2430 - val_acc: 0.3651\n"
     ]
    }
   ],
   "source": [
    "# set epochs + batch size\n",
    "NUM_EPOCHS = 100\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(validation_features, validation_labels),\n",
    "                    verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.8798 - acc: 0.4513 - val_loss: 2.2240 - val_acc: 0.3763\n",
      "Epoch 2/200\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 1.8724 - acc: 0.4570 - val_loss: 2.2284 - val_acc: 0.3811\n",
      "Epoch 3/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.8671 - acc: 0.4618 - val_loss: 2.2507 - val_acc: 0.3690\n",
      "Epoch 4/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.8592 - acc: 0.4614 - val_loss: 2.2474 - val_acc: 0.3824\n",
      "Epoch 5/200\n",
      "20000/20000 [==============================] - 197s 10ms/step - loss: 1.8564 - acc: 0.4635 - val_loss: 2.2247 - val_acc: 0.3769\n",
      "Epoch 6/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.8570 - acc: 0.4638 - val_loss: 2.2357 - val_acc: 0.3793\n",
      "Epoch 7/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.8513 - acc: 0.4639 - val_loss: 2.2105 - val_acc: 0.3845\n",
      "Epoch 8/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.8528 - acc: 0.4642 - val_loss: 2.2297 - val_acc: 0.3777\n",
      "Epoch 9/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.8474 - acc: 0.4677 - val_loss: 2.2188 - val_acc: 0.3757\n",
      "Epoch 10/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.8402 - acc: 0.4660 - val_loss: 2.2351 - val_acc: 0.3792\n",
      "Epoch 11/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.8346 - acc: 0.4676 - val_loss: 2.2174 - val_acc: 0.3855\n",
      "Epoch 12/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.8311 - acc: 0.4689 - val_loss: 2.2218 - val_acc: 0.3865\n",
      "Epoch 13/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.8365 - acc: 0.4694 - val_loss: 2.2201 - val_acc: 0.3801\n",
      "Epoch 14/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.8331 - acc: 0.4681 - val_loss: 2.2500 - val_acc: 0.3699\n",
      "Epoch 15/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.8202 - acc: 0.4701 - val_loss: 2.2775 - val_acc: 0.3751\n",
      "Epoch 16/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.8171 - acc: 0.4741 - val_loss: 2.2213 - val_acc: 0.3824\n",
      "Epoch 17/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.8163 - acc: 0.4733 - val_loss: 2.2514 - val_acc: 0.3724\n",
      "Epoch 18/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.8157 - acc: 0.4726 - val_loss: 2.2287 - val_acc: 0.3767\n",
      "Epoch 19/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.8089 - acc: 0.4739 - val_loss: 2.2751 - val_acc: 0.3599\n",
      "Epoch 20/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.8080 - acc: 0.4775 - val_loss: 2.3365 - val_acc: 0.3721\n",
      "Epoch 21/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.8015 - acc: 0.4758 - val_loss: 2.2228 - val_acc: 0.3835\n",
      "Epoch 22/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.7974 - acc: 0.4728 - val_loss: 2.2260 - val_acc: 0.3853\n",
      "Epoch 23/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.8018 - acc: 0.4754 - val_loss: 2.2779 - val_acc: 0.3787\n",
      "Epoch 24/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.7925 - acc: 0.4775 - val_loss: 2.2285 - val_acc: 0.3803\n",
      "Epoch 25/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.7922 - acc: 0.4792 - val_loss: 2.2853 - val_acc: 0.3818\n",
      "Epoch 26/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7896 - acc: 0.4813 - val_loss: 2.2348 - val_acc: 0.3841\n",
      "Epoch 27/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.7861 - acc: 0.4807 - val_loss: 2.2216 - val_acc: 0.3827\n",
      "Epoch 28/200\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 1.7789 - acc: 0.4800 - val_loss: 2.2304 - val_acc: 0.3822\n",
      "Epoch 29/200\n",
      "20000/20000 [==============================] - 197s 10ms/step - loss: 1.7822 - acc: 0.4812 - val_loss: 2.2685 - val_acc: 0.3649\n",
      "Epoch 30/200\n",
      "20000/20000 [==============================] - 198s 10ms/step - loss: 1.7723 - acc: 0.4860 - val_loss: 2.2170 - val_acc: 0.3817\n",
      "Epoch 31/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.7671 - acc: 0.4865 - val_loss: 2.2167 - val_acc: 0.3878\n",
      "Epoch 32/200\n",
      "20000/20000 [==============================] - 198s 10ms/step - loss: 1.7656 - acc: 0.4895 - val_loss: 2.2384 - val_acc: 0.3767\n",
      "Epoch 33/200\n",
      "20000/20000 [==============================] - 198s 10ms/step - loss: 1.7627 - acc: 0.4895 - val_loss: 2.2651 - val_acc: 0.3779\n",
      "Epoch 34/200\n",
      "20000/20000 [==============================] - 193s 10ms/step - loss: 1.7636 - acc: 0.4866 - val_loss: 2.2570 - val_acc: 0.3678\n",
      "Epoch 35/200\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 1.7561 - acc: 0.4901 - val_loss: 2.2280 - val_acc: 0.3802\n",
      "Epoch 36/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7479 - acc: 0.4906 - val_loss: 2.2501 - val_acc: 0.3821\n",
      "Epoch 37/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.7491 - acc: 0.4907 - val_loss: 2.2304 - val_acc: 0.3861\n",
      "Epoch 38/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.7451 - acc: 0.4928 - val_loss: 2.2364 - val_acc: 0.3852\n",
      "Epoch 39/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7423 - acc: 0.4919 - val_loss: 2.2201 - val_acc: 0.3911\n",
      "Epoch 40/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.7380 - acc: 0.4954 - val_loss: 2.2498 - val_acc: 0.3849\n",
      "Epoch 41/200\n",
      "20000/20000 [==============================] - 198s 10ms/step - loss: 1.7356 - acc: 0.4952 - val_loss: 2.2291 - val_acc: 0.3831\n",
      "Epoch 42/200\n",
      "20000/20000 [==============================] - 206s 10ms/step - loss: 1.7364 - acc: 0.4942 - val_loss: 2.2630 - val_acc: 0.3808\n",
      "Epoch 43/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.7240 - acc: 0.4966 - val_loss: 2.2251 - val_acc: 0.3861\n",
      "Epoch 44/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.7254 - acc: 0.4990 - val_loss: 2.2632 - val_acc: 0.3831\n",
      "Epoch 45/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7221 - acc: 0.4978 - val_loss: 2.2330 - val_acc: 0.3821\n",
      "Epoch 46/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.7194 - acc: 0.4993 - val_loss: 2.2509 - val_acc: 0.3742\n",
      "Epoch 47/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7139 - acc: 0.5001 - val_loss: 2.2508 - val_acc: 0.3852\n",
      "Epoch 48/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7156 - acc: 0.5008 - val_loss: 2.2425 - val_acc: 0.3866\n",
      "Epoch 49/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.7143 - acc: 0.5027 - val_loss: 2.2455 - val_acc: 0.3841\n",
      "Epoch 50/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.7111 - acc: 0.5037 - val_loss: 2.2246 - val_acc: 0.3898\n",
      "Epoch 51/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.7056 - acc: 0.5016 - val_loss: 2.2486 - val_acc: 0.3722\n",
      "Epoch 52/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.6977 - acc: 0.5036 - val_loss: 2.2648 - val_acc: 0.3860\n",
      "Epoch 53/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.6919 - acc: 0.5042 - val_loss: 2.2362 - val_acc: 0.3889\n",
      "Epoch 54/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.6912 - acc: 0.5067 - val_loss: 2.2335 - val_acc: 0.3899\n",
      "Epoch 55/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6910 - acc: 0.5061 - val_loss: 2.2254 - val_acc: 0.3926\n",
      "Epoch 56/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.6877 - acc: 0.5101 - val_loss: 2.2478 - val_acc: 0.3754\n",
      "Epoch 57/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.6863 - acc: 0.5082 - val_loss: 2.2748 - val_acc: 0.3851\n",
      "Epoch 58/200\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 1.6818 - acc: 0.5105 - val_loss: 2.2382 - val_acc: 0.3865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.6761 - acc: 0.5133 - val_loss: 2.2334 - val_acc: 0.3897\n",
      "Epoch 60/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6777 - acc: 0.5111 - val_loss: 2.2285 - val_acc: 0.3902\n",
      "Epoch 61/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.6708 - acc: 0.5172 - val_loss: 2.2384 - val_acc: 0.3876\n",
      "Epoch 62/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.6658 - acc: 0.5145 - val_loss: 2.2461 - val_acc: 0.3768\n",
      "Epoch 63/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.6672 - acc: 0.5107 - val_loss: 2.2593 - val_acc: 0.3750\n",
      "Epoch 64/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.6650 - acc: 0.5127 - val_loss: 2.2759 - val_acc: 0.3846\n",
      "Epoch 65/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.6630 - acc: 0.5122 - val_loss: 2.2444 - val_acc: 0.3821\n",
      "Epoch 66/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6592 - acc: 0.5145 - val_loss: 2.2597 - val_acc: 0.3836\n",
      "Epoch 67/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6518 - acc: 0.5218 - val_loss: 2.2479 - val_acc: 0.3818\n",
      "Epoch 68/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.6464 - acc: 0.5222 - val_loss: 2.2460 - val_acc: 0.3898\n",
      "Epoch 69/200\n",
      "20000/20000 [==============================] - 195s 10ms/step - loss: 1.6437 - acc: 0.5200 - val_loss: 2.2329 - val_acc: 0.3904\n",
      "Epoch 70/200\n",
      "20000/20000 [==============================] - 198s 10ms/step - loss: 1.6424 - acc: 0.5223 - val_loss: 2.2473 - val_acc: 0.3890\n",
      "Epoch 71/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.6428 - acc: 0.5203 - val_loss: 2.3996 - val_acc: 0.3662\n",
      "Epoch 72/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6377 - acc: 0.5207 - val_loss: 2.3147 - val_acc: 0.3814\n",
      "Epoch 73/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6363 - acc: 0.5236 - val_loss: 2.2401 - val_acc: 0.3901\n",
      "Epoch 74/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.6311 - acc: 0.5210 - val_loss: 2.2947 - val_acc: 0.3856\n",
      "Epoch 75/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.6265 - acc: 0.5199 - val_loss: 2.2665 - val_acc: 0.3738\n",
      "Epoch 76/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.6264 - acc: 0.5235 - val_loss: 2.2362 - val_acc: 0.3895\n",
      "Epoch 77/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.6287 - acc: 0.5222 - val_loss: 2.2532 - val_acc: 0.3937\n",
      "Epoch 78/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.6305 - acc: 0.5220 - val_loss: 2.2656 - val_acc: 0.3776\n",
      "Epoch 79/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.6216 - acc: 0.5260 - val_loss: 2.2484 - val_acc: 0.3918\n",
      "Epoch 80/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.6129 - acc: 0.5290 - val_loss: 2.2984 - val_acc: 0.3884\n",
      "Epoch 81/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.6139 - acc: 0.5338 - val_loss: 2.2974 - val_acc: 0.3907\n",
      "Epoch 82/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.6077 - acc: 0.5311 - val_loss: 2.3037 - val_acc: 0.3863\n",
      "Epoch 83/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.6078 - acc: 0.5319 - val_loss: 2.2316 - val_acc: 0.3902\n",
      "Epoch 84/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.5967 - acc: 0.5309 - val_loss: 2.2969 - val_acc: 0.3822\n",
      "Epoch 85/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5979 - acc: 0.5315 - val_loss: 2.2408 - val_acc: 0.3910\n",
      "Epoch 86/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5996 - acc: 0.5342 - val_loss: 2.2439 - val_acc: 0.3913\n",
      "Epoch 87/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5997 - acc: 0.5301 - val_loss: 2.2621 - val_acc: 0.3830\n",
      "Epoch 88/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.5953 - acc: 0.5354 - val_loss: 2.2554 - val_acc: 0.3864\n",
      "Epoch 89/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.5893 - acc: 0.5355 - val_loss: 2.2748 - val_acc: 0.3868\n",
      "Epoch 90/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5858 - acc: 0.5347 - val_loss: 2.2414 - val_acc: 0.3879\n",
      "Epoch 91/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.5841 - acc: 0.5372 - val_loss: 2.2565 - val_acc: 0.3900\n",
      "Epoch 92/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5790 - acc: 0.5380 - val_loss: 2.2642 - val_acc: 0.3885\n",
      "Epoch 93/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5747 - acc: 0.5403 - val_loss: 2.2981 - val_acc: 0.3824\n",
      "Epoch 94/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.5772 - acc: 0.5393 - val_loss: 2.3080 - val_acc: 0.3736\n",
      "Epoch 95/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.5688 - acc: 0.5410 - val_loss: 2.3226 - val_acc: 0.3826\n",
      "Epoch 96/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.5678 - acc: 0.5397 - val_loss: 2.2657 - val_acc: 0.3924\n",
      "Epoch 97/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5604 - acc: 0.5473 - val_loss: 2.3108 - val_acc: 0.3890\n",
      "Epoch 98/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5649 - acc: 0.5469 - val_loss: 2.3386 - val_acc: 0.3662\n",
      "Epoch 99/200\n",
      "20000/20000 [==============================] - 206s 10ms/step - loss: 1.5613 - acc: 0.5441 - val_loss: 2.2930 - val_acc: 0.3890\n",
      "Epoch 100/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5578 - acc: 0.5420 - val_loss: 2.3684 - val_acc: 0.3808\n",
      "Epoch 101/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5571 - acc: 0.5449 - val_loss: 2.2717 - val_acc: 0.3882\n",
      "Epoch 102/200\n",
      "20000/20000 [==============================] - 206s 10ms/step - loss: 1.5477 - acc: 0.5487 - val_loss: 2.2698 - val_acc: 0.3946\n",
      "Epoch 103/200\n",
      "20000/20000 [==============================] - 199s 10ms/step - loss: 1.5463 - acc: 0.5456 - val_loss: 2.3380 - val_acc: 0.3906\n",
      "Epoch 104/200\n",
      "20000/20000 [==============================] - 196s 10ms/step - loss: 1.5471 - acc: 0.5495 - val_loss: 2.3049 - val_acc: 0.3636\n",
      "Epoch 105/200\n",
      "20000/20000 [==============================] - 197s 10ms/step - loss: 1.5492 - acc: 0.5465 - val_loss: 2.2748 - val_acc: 0.3899\n",
      "Epoch 106/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.5388 - acc: 0.5497 - val_loss: 2.4041 - val_acc: 0.3783\n",
      "Epoch 107/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5408 - acc: 0.5463 - val_loss: 2.2910 - val_acc: 0.3902\n",
      "Epoch 108/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5314 - acc: 0.5519 - val_loss: 2.2645 - val_acc: 0.3937\n",
      "Epoch 109/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5303 - acc: 0.5538 - val_loss: 2.2802 - val_acc: 0.3886\n",
      "Epoch 110/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5237 - acc: 0.5559 - val_loss: 2.3244 - val_acc: 0.3872\n",
      "Epoch 111/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.5207 - acc: 0.5570 - val_loss: 2.2883 - val_acc: 0.3850\n",
      "Epoch 112/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.5199 - acc: 0.5543 - val_loss: 2.2811 - val_acc: 0.3947\n",
      "Epoch 113/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5192 - acc: 0.5568 - val_loss: 2.2924 - val_acc: 0.3848\n",
      "Epoch 114/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5184 - acc: 0.5593 - val_loss: 2.3032 - val_acc: 0.3927\n",
      "Epoch 115/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5103 - acc: 0.5590 - val_loss: 2.2747 - val_acc: 0.3868\n",
      "Epoch 116/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.5117 - acc: 0.5562 - val_loss: 2.2953 - val_acc: 0.3881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.5085 - acc: 0.5585 - val_loss: 2.3171 - val_acc: 0.3840\n",
      "Epoch 118/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.5146 - acc: 0.5587 - val_loss: 2.3277 - val_acc: 0.3671\n",
      "Epoch 119/200\n",
      "20000/20000 [==============================] - 202s 10ms/step - loss: 1.5002 - acc: 0.5630 - val_loss: 2.3335 - val_acc: 0.3903\n",
      "Epoch 120/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.5032 - acc: 0.5610 - val_loss: 2.2955 - val_acc: 0.3925\n",
      "Epoch 121/200\n",
      "20000/20000 [==============================] - 201s 10ms/step - loss: 1.4978 - acc: 0.5614 - val_loss: 2.3421 - val_acc: 0.3879\n",
      "Epoch 122/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.4923 - acc: 0.5613 - val_loss: 2.3131 - val_acc: 0.3804\n",
      "Epoch 123/200\n",
      "20000/20000 [==============================] - 200s 10ms/step - loss: 1.4991 - acc: 0.5644 - val_loss: 2.3339 - val_acc: 0.3899\n",
      "Epoch 124/200\n",
      "20000/20000 [==============================] - 203s 10ms/step - loss: 1.4871 - acc: 0.5646 - val_loss: 2.3134 - val_acc: 0.3887\n",
      "Epoch 125/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.4873 - acc: 0.5682 - val_loss: 2.2995 - val_acc: 0.3911\n",
      "Epoch 126/200\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 1.4833 - acc: 0.5662 - val_loss: 2.3021 - val_acc: 0.3861\n",
      "Epoch 127/200\n",
      "20000/20000 [==============================] - 205s 10ms/step - loss: 1.4777 - acc: 0.5688 - val_loss: 2.3971 - val_acc: 0.3477\n",
      "Epoch 128/200\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 1.4744 - acc: 0.5646"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-79ea28b633c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     verbose=1, callbacks=[tensorboard])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set epochs + batch size\n",
    "NUM_EPOCHS = 200\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_data=(validation_features, validation_labels),\n",
    "                    verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
